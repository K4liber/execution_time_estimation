%!TEX root = main.tex
\section{Algorithms}
\subsection{Support Vector Regression}

As a one of the machine learning algorithms to estimate time of a CAL module execution we will use \textit{Epsilon-Support Vector Regression}~\cite{svrc} (SVR) - it is based on Support Vector Machine (SVM, orginally named \textit{support-vector networks}\cite{svm}).

We will use the SVR model with an RBF kernel\cite{rbf_kernel} which is the most known flexible kernel and it could project the features vectors into infinite dimensions. It uses Taylor expansion which is equivalent to use an infinite sum over the polynomial kernels. It allows modeling any function that is a sum of unknown degree polynomials.

Using kernel, the resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The RBF kernel have the following form:

\[ K_{RBF}(\vec{x}, \vec{x}') = \exp{(-\gamma||\vec{x} - \vec{x}'||)}\ \textnormal{, where:}\]
\begin{itemize}
	\item $ ||\vec{x} - \vec{x}'|| $ is the squared Euclidean distance between the two vectors of features,\newline
	\item $ \gamma $ - hyperparameter described in more details below.
\end{itemize}
In the SVR algorithm we are looking for a hyperplane \textit{y} in the following form:
\[ y = \vec{w} \vec{x} + b \textnormal{, where:}\]
\begin{itemize}
	\item $ \vec{x} $ - vector of features,
	\item $ \vec{w} $ - normal vector to the hyperplane \textit{y}, using a kernel, the $ \vec{w} $ is also in the transformed space.
\end{itemize}
Training the original SVR means solving:
\[ \frac{1}{2}||w||^2  + C \sum_{i}^{N}(\xi_i+\xi_i^*) \textnormal{,}\]
with the following constraints:
\[ y_i - \vec{w}x_i - b \le \epsilon + \xi_i  \]
\[ -y_i + \vec{w}x_i + b \le \epsilon + \xi_i^*  \]
\[ \xi_i \xi_i^* \ge 0 \]
Figure~\ref{fig:svr_epsilon_gamma} shows the example of a hyperplane with marked \textit{$\xi$} and \textit{$\epsilon$} parameters.
As we have already chosen the \textit{RBF} kernel for the \textit{SVR} algorithm, our modeling is simplified just to find the best values of the following hyperparameters\cite{svr}:
\begin{enumerate}
	\item \textit{C} -the weight of an error cost. The regularization\footnote{regularization is a way to give a penalty to certain models (usually overly complex ones)} hyperparameters have to be strictly positive. The example from the figure use l1 penalty (the used modeling library uses the squared epsilon-insensitive loss with l2 penalty). The strength of the regularization is inversely proportional to C. The larger value of C, the more variance is introduced into the model. 
	\item \textit{epsilon} - It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. As it is shown of Figure~\ref{fig:svr_epsilon_gamma} the green data points do not provide any penalty to the loss function because they are within the allowed epsilon range around the approximation\footnote{\label{figure_example}The figure is based on some example data and it only shows the hyperparameter influence on model.}.
	\item \textit{gamma} - The \textit{gamma} hyperparameter can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. Increasing the value of \textit{gamma} hyperparameter causes the variance increase what is shown in the figure~\ref{fig:gamma}\footnotemark[\getcountref{figure_example}].
	
\end{enumerate}
\begin{figure}[!htb]
	\caption{Visualization of the SVR's epsilon and gamma parameters~\citeinside{epsilon}.}
	\centering
	\label{fig:svr_epsilon_gamma}
	\includegraphics[width=0.5\textwidth]{svr_epsilon_gamma}
\end{figure}
\begin{figure}[!htb]
	\caption{SVR's gamma hyperparameter~\citeinside{gamma} influence on the model variance.}
	\centering
	\label{fig:gamma}
	\includegraphics[width=0.45\textwidth]{svr_gamma}
\end{figure}
You can find detailed information about C and gamma params in sklearn library documentation\cite{rbf_params}.

\begin{figure*}[!t]
	\centering
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=1.0\textwidth]{svr_out_of_range}
	\end{minipage}
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=1.0\textwidth]{knn_out_of_range}
	\end{minipage}
	\caption{Regression on the out of training range data.}
	\label{fig:out_of_range}
\end{figure*}

\subsection{K-nearest Neighbors Regression}

Another, examined algorithm used to estimating time of a module execution is a simple \textit{KNN}\footnote{\textit{KNN} - k-nearest neighbors} regression. The target is predicted by local interpolation of the targets associated with the nearest neighbors in the training set~\cite{knnreg}.In other words, the target assigned to a query point is computed based on the mean of the targets of its nearest neighbors.

To use the algorithm, we have to define values of the following parameters:
\begin{enumerate}
	\item \textit{k} - number of the nearest neighbors
	\item \textit{weights} - weight function used in prediction. The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. The weights can be calculated from the distances using any function the linear one, for example.
	\item \textit{algorithm} - the procedure to calculate k-nearest neighbors for the query point. It does not have a direct impact to the final regression result, but the parameters of the algorithm surely have. For example, using the BallTree algorithm, we have to choose the \textit{metric} parameter that will be used to calculate the distance between data points. It could be, for example, the Minkowski~\cite{minkowski} metric with the l2 (standard Euclidean~\cite{euclidean}) distance metric or any function that will calculate the distance between two points.
\end{enumerate}

\subsection{Perceptions}

 The problem we are trying to solve is a small-p regression type. Our models are based on only a few input features, so the examined space is low-dimensional. We do not need complex machine learning tools like XGBoost~\cite{xgb} or neural networks to receive satisfactory results. The algorithms described above was selected for modeling because of their simplicity. 
 
 We assume that most of the modules have polynomial time complexity. The \textit{SVR} is a natural choice for the type of problem we are trying to solve because of its ability to model any function that is a combination of any polynomials. On the other hand, we decided to use the \textit{KNN} as well. 
 
 The simplest way to predict a module run-time is to look at the historical executions of a module with the input data that have similar properties to the executing one. That is what the \textit{KNN} algorithm does. A huge disadvantage of the algorithm is a lack of ability to provide acceptable estimations for outstanding data points from the training data points. The problem is visualized in the Figure~\ref{fig:out_of_range}. It presents the regression on the example input data. The \textit{KNN} cannot predict the outstanding samples correctly.
 